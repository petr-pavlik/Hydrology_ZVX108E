[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ZVX108E Hydrology",
    "section": "",
    "text": "Introduction\nThe ZVX108E Hydrology taught at the Czech University of Life Sciences is an introductory undergraduate course with students of various scientific background, many of whom do not possess knowledge of any programming language yet. Hence we introduce the R programming language which serves for decades as a great tool for scientific data processing, statistical evaluation, visualization and reporting. The choice of R over other tools is opinionated. R is told to be “developed by statisticians for statisticians” and such as fits perfectly well into the workflow of hydrological data processing. We believe it is more straightforward to learn than Python and does not main students with plethora of environments and incomparable versions. There is a saying “Python is the second best language for everything” and we as authors fully agree. Since the course only contains six practical sessions, oriented to various parts of hydrology, we focus on the basic and merit.\nThis text was created with the use of 4.3.0 and namespaces of following packages:\nVersion\nbase          4.3.0\ncli           3.6.1\ncompiler      4.3.0\ndatasets      4.3.0\ndigest       0.6.33\nevaluate       0.21\nfastmap       1.1.1\nglue          1.6.2\ngraphics      4.3.0\ngrDevices     4.3.0\nhtmltools     0.5.6\nhtmlwidgets   1.6.2\njsonlite      1.8.7\nknitr          1.43\nlifecycle     1.0.3\nmagrittr      2.0.3\nmethods       4.3.0\nrlang         1.1.1\nrmarkdown      2.24\nrstudioapi     0.14\nstats         4.3.0\nstringi      1.7.12\nstringr       1.5.0\ntools         4.3.0\nutils         4.3.0\nxfun           0.40\nReproduction of all the materials should be possible using the same versions."
  },
  {
    "objectID": "index.html#data-and-companion-structure",
    "href": "index.html#data-and-companion-structure",
    "title": "ZVX108E Hydrology",
    "section": "Data and companion structure",
    "text": "Data and companion structure"
  },
  {
    "objectID": "intro.html#recommended-literature",
    "href": "intro.html#recommended-literature",
    "title": "1  Data download",
    "section": "1.1 Recommended literature",
    "text": "1.1 Recommended literature\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "01_R.html#r-as-scientific-calculator",
    "href": "01_R.html#r-as-scientific-calculator",
    "title": "2  Introduction to R language",
    "section": "2.1 R as scientific calculator",
    "text": "2.1 R as scientific calculator\n\nArithmetic operations\n\n\nCode\n1 + 2           # addition\n## [1] 3\n1 - 2           # subtraction \n## [1] -1\n1 / 2           # division\n## [1] 0.5\n1 * 2           # multiplication\n## [1] 2\n1 %/% 2         # integer division\n## [1] 0\n1 %% 2          # modulo oprator\n## [1] 1\n\n\n\n\nSpecial values\nR is familiar with the concept of \\(\\pm\\infty\\), hence -Inf and Inf values are at disposal. You will get them most probably as results from computation heading to \\(\\frac{\\pm1}{0}\\) numerically. There are other special values like NULL (null value), NA (not assigned) and NaN (not a number). The concept of not assigned is one that is particularly important, since it has significant impact on the computed result ({(code-mean-rm?)}).\n\n\nCode\nx &lt;- seq(1:10)             # general sequence of numbers\nx[c(5,6)] &lt;- NA            # change some elements to not assigned\nprint(x)\n\n\n [1]  1  2  3  4 NA NA  7  8  9 10\n\n\nCode\nmean(x)                    # without removal\n\n\n[1] NA\n\n\nCode\nmean(x, na.rm = TRUE)      # and with removal\n\n\n[1] 5.5\n\n\n\n\nSet operations\nFor manipulating sets, there are a couple of essential functions union(), intersect(), setdiff() and operator %in%.\n\n\nCode\nset_A &lt;- c(\"a\", \"a\", \"b\", \"c\", \"D\")\nset_B &lt;- c(\"a\", \"b\", \"d\")\nunion(set_A, set_B)\n## [1] \"a\" \"b\" \"c\" \"D\" \"d\"\nintersect(set_A, set_B)\n## [1] \"a\" \"b\"\nset_A %in% set_B\n## [1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\n\n\nMatrix operations\nFor the purpose of following examples let’s use an arbitrary matrix \\(M\\) and a vectors \\(U\\) and \\(V\\).\n\\[\nA = \\left(\\begin{matrix}\n2x& - 3y& &= 3\\\\\n& - 2y& + 4z &= 9\\\\\n2x& + 13y& + 9z&= 10\n\\end{matrix}\\right),\\\\\n\\tag{2.1}\\]\n\\[\nU = \\begin{pmatrix}\n1\\\\\n-3\\\\\n8\\\\\n\\end{pmatrix},\nV = \\begin{pmatrix}\n1\\\\\n-3\\\\\n8\\\\\n\\end{pmatrix}\n\\tag{2.2}\\]\nSolving a system of linear equations {Equation 2.1} is a one-liner:\n\n\nCode\nA &lt;- matrix(data = c(2, -3, 0, 0, -2, 4, 2, 13, 9), nrow = 3, byrow = TRUE)\nB &lt;- c(3, 9, 10)\nsolve(A, B)\n\n\n[1]  0.5304878 -0.6463415  1.9268293"
  },
  {
    "objectID": "01_R.html#r-as-programming-language",
    "href": "01_R.html#r-as-programming-language",
    "title": "2  Introduction to R language",
    "section": "2.2 R as programming language",
    "text": "2.2 R as programming language\n\n2.2.1 Variables and name conventions\nIt is possible. We highly discourage using diacritical marks in naming, like the Czech translation of the term “variable” - proměnná. Most programmers use either camelNotation or snake_notation for naming purposes. Obviously the R is case-sensitive so camelNotation and CamelNotation are two different things. Variables do not contain spaces, quotes, arithmetical, logical nor relational operators neither they contain special characters like =, -, ``.\n\n\n2.2.2 Functions\nYou can define own functions using the function() construct. If you work in ****RStudio, just type fun and tabulate a snippet from the IDE help. The action produces {(code-function-snippet?)}.\n\n\nCode\nname &lt;- function(variables) {\n  ...\n}\n\n\nname is the name of the function we would like to create and variables are the arguments of that function. Space between the {and } is called a body of a function and contains all the computation which is invoked when the function is called.\nLet’s put Here an example of creating own function to calculate weighted mean\n\\[\n\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} x_iw_i,\n\\] where \\(x_iw_i\\) are the individual weighted measurements.\nWe define a simple function for that purpose and run an example.\n\n\nCode\nw_mean &lt;- function(x, w = 1/length(x)) {\n  1/1/length(x)*sum(x)\n}\nw_mean(1:10)\n\n\n[1] 5.5\n\n\nWe can test if we get the same result as the primitive function from R using all.equal() statement.\n\n\nCode\nall.equal(w_mean(x = 1:10), \n          weighted.mean(x = 1:10, w = rep(x = 1/10, times = 10)))\n\n\n[1] TRUE\n\n\nAny argument without default value in the function definition has to be provided on function call. You can frequently see functions with the possibility to specify ... a so-called three dot construct or ellipsis. The ellipsis allows for adding any number of arguments to a function call, after all the named ones.\n\n\n2.2.3 Data types\nThe basic types are logical, integer, numeric, complex, character and raw. There are some additional types which we will encounter like Date. Since R is dynamically typed, it is not necessary for the user to declare variables before using them. Also the type changes without notice based on the stored values, where the chain goes from the least complex to the most. The summary is in the following table\n\n\nCode\nTRUE    # logical, also T as short version\n## [1] TRUE\n1L      # integer\n## [1] 1\n1.2     # numeric\n## [1] 1.2\n1+3i    # complex\n## [1] 1+3i\n\"A\"     # character, also 'A'\n## [1] \"A\"\n\n\n\n\n2.2.4 Data structures\n\nVectors\nAtomic vectors are single-type linear structures. They can contain elements of any type, from logical, integer, numeric, complex, character.\n\n\nCode\n```{r}\n#| label: test-code-annotation\nV &lt;- vector(mode = \"numeric\", length = 0) # empty numeric vector creation\nV[1] &lt;- \"A\"\n```\n\n\n\n\nMatrices and arrays\nIf the object has more than one dimension, it is treated as an array. A special type of array is a matrix. Both object types have accompanying functions like colSums(), rowMeans().\n\n\nCode\nM &lt;- matrix(data = 0, nrow = 5, ncol = 2) # empty matrix creation\nM[1, 1] &lt;- 1                              # add single value at origin\nM[, 1] &lt;- 1.5                             # store 1.5 to the whole first column\nM[c(1,3), 1:2] &lt;- rnorm(2)                # store random numbers to first two rows\n\ncolMeans(M) \n## [1] 1.3516954 0.4516954\nrowSums(M)\n## [1] 1.449950 1.500000 3.067004 1.500000 1.500000\n\n\nIt is possible to have matrices containing any data type, e.g.\n\\[\nM = \\left(\\begin{matrix}\n\\mathrm{A} & \\mathrm{B}\\\\\n\\mathrm{C} & \\mathrm{D}\n\\end{matrix}\\right),\\qquad\nN = \\left(\\begin{matrix}\n1+i & 5-3i\\\\\n10+2i & i\n\\end{matrix}\\right)\n\\]\n\n\nData frames\ndata.frame structure is the workhorse of elementary data processing. It is a possibly heterogenic table-like structure, allowing storage of multiple data types (even other structures) in different columns. A column in any data frame is called a variable and row represents a single observation. If the data suffice this single condition, we say they are in tidy format. Processing tidy data is a big topic withing the R community and curious reader is encouraged to follow the development in tidyverse package ecosystem.\n\n\nCode\nthaya &lt;- data.frame(date = NA, \n                    runoff = NA, \n                    precipitation = NA) # new empty data.frame with variables 'date', 'runoff', 'precipitation' and 'temperature'\n#thaya$runoff &lt;- rnorm(100, 1, 2)\n\n\n\n\nLists\nList is the most general basic data structure. It is possible to store vectors, matrices, data frames and also other lists within a list. List structure does not pose any limitations on the internal objects lengths.\n\n\nCode\nl &lt;- list() # empty list creation \nl[\"A\"] &lt;- 1\nprint(l)\n\n\n$A\n[1] 1\n\n\nCode\nl$A &lt;- 2\nprint(l)\n\n\n$A\n[1] 2\n\n\n\n\nOther objects\nAlthough R is intended as functional programming language, more than one object oriented paradigm is implemented in the language. As new R users we encounter first OOP system in functions like summary and plot, which represent so called S3 generic functions. We will further work with S4 system when processing geospatial data using proxy libraries like sf and terra. The OOP is very complex and will not be further discussed within this text. For further study we recommend OOP sections in Advanced R by Hadley Wickham.\n\n\n\n2.2.5 Conditions\nA condition in code creates branching of computation. Placing a condition creates at least two options from which only one is to be satisfied. The condition is created either by if()/ifelse() or switch() construct. We can again call for a snippet from RStudio help resulting in\n\n\nCode\nif (condition) {\n  ...\n}\n\nswitch (object,\n  case = action\n)\n\nifelse(test, TRUE, FALSE)\n\n\nThe condition can be branched to larger structures like\n\n\nCode\ntemperature &lt;- 30\nif (temperature &gt; 30) {\n  cat(\"The temperature is hot.\")\n} else if (temperature &gt; 15) {\n  cat(\"The temperature is warm.\")\n}\n\n\nThe temperature is warm.\n\n\n\n\n2.2.6 Repetition\nLoops (cycles) provide use with the ability to execute single statement of a block of code in {} multiple times. There are three key words for loop construction. They differ in use cases.\n\n2.2.6.1 for cycle\nProbably the most common loop is used when you know the number of iterations prior to calling. The iteration is therefore explicitly finite.\n\n\nCode\nfor (variable in vector) {\n  ...\n}\n\n\nAn example\n\n\nCode\nfor(i in 1:4) cat(i, \". iteration\", \"\\n\", sep = \"\")\n\n\n1. iteration\n2. iteration\n3. iteration\n4. iteration\n\n\n\n\n2.2.6.2 while cycle\nwhile is used in when it is impossible to state how many times something should be repeated. The case is rather in the form while some condition is or is not met, repeat what is inside the body. It is also used in intentionally infinite loop e.g. operating systems.\n\n\n2.2.6.3 repeat cycle\nIn the cases when we need the repetition at least once, we will evaluate the code inside until a condition is met.\n\n\nCode\nrepeat({\n  x &lt;- rnorm(1)\n  cat(x)\n  if(x &gt; 0) break\n})\n\n\n-0.32389592.105889\n\n\n\n\n2.2.6.4 break and next\nThere are two statements which controls the iteration flow. Anytime break is called, the rest of the body is skipped and the loop ends. Anytime next is called, the rest of the body is skipped and next iteration is started.\n\n\nI would like to have text here\nSentence becomes longer, it should automatically stay in their column\n\n\n\nand here\nMore text\n\n\n\n\n\n2.2.7 Exercise\n\ncreate a sequence of numbers calling\nWhat type is NA, why would you say is it?"
  },
  {
    "objectID": "02_statistics.html#exploratory-data-analysis-eda",
    "href": "02_statistics.html#exploratory-data-analysis-eda",
    "title": "3  Statistical processing of hydrological data",
    "section": "3.1 Exploratory Data Analysis (EDA)",
    "text": "3.1 Exploratory Data Analysis (EDA)\nWhen studying data set, we will look at certain statistical features, representative for the whole data set.\n\n3.1.1 Measures of location\n\n3.1.1.1 Mean\n\\[\n\\bar{x} = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} x_i\n\\]\n\n\n\n3.1.2 Measures of variability\n\\[\n\\sigma = \\dfrac{\\sqrt{(x_i - \\bar{x})^2}}{n-1}\n\\]"
  },
  {
    "objectID": "02_statistics.html#hydrological-data",
    "href": "02_statistics.html#hydrological-data",
    "title": "3  Statistical processing of hydrological data",
    "section": "3.2 Hydrological data",
    "text": "3.2 Hydrological data\nDatasets containing hydrological data are most commonly, although not exclusively, in tabular (rectangular) shape.\nLet’s take a look at data from CAMELS (Addor et al. 2017) at https://gdex.ucar.edu/dataset/camels.html. It is a curated large sample data set, which was produced by the UCAR with the intention to\nThis dataset covers the same 671 catchments as the Large-Sample Hydrometeorological dataset introduced by Newman et al. (2015). A wide range of attributes accompanies each catchment that influence catchment behavior and hydrological processes. Datasets characterizing these attributes have been available separately for some time, but comprehensive multivariate catchment scale assessments have so far been difficult, because these datasets typically have different spatial configurations, are stored in different archives, or use different data formats. By creating catchment scale estimates of these attributes, our aim is to simplify the assessment of their interrelationships.\nTopographic characteristics (e.g. elevation and slope) were retrieved from Newman et al. (2015). Climatic indices (e.g., aridity and frequency of dry days) and hydrological signatures (e.g., mean annual discharge and baseflow index) were computed using the time series provided by Newman et al. (2015). Soil characteristics (e.g., porosity and soil depth) were characterized using the STATSGO dataset and the Pelletier et al. (2016) dataset. Vegetation characteristics (e.g. the leaf area index and the rooting depth) were inferred using MODIS data. Geological characteristics (e.g., geologic class and the subsurface porosity) were computed using the GLiM and GLHYMPS datasets.\n\n\nCode\n#ts &lt;- readr::read_csv(\"data/\")"
  },
  {
    "objectID": "02_statistics.html#data",
    "href": "02_statistics.html#data",
    "title": "3  Statistical processing of hydrological data",
    "section": "3.3 Data",
    "text": "3.3 Data\nLet’s start with processing the data, that we will use within this chapter\n\n\nCode\ndata_03463300 &lt;- read.fwf(file = \"./data/03463300_streamflow_qc.txt\", \n                          widths = c(8, 4, 2, 2, 8, 4), \n                          header = FALSE)\n\nnames(data_03463300) &lt;- c(\"id\", \"yr\", \"mon\", \"day\", \"discharge\", \"cat\")\n\ndata_03463300$mon &lt;- gsub(x = as.character(data_03463300$mon), \n                          pattern = \" \", \n                          replacement = \"0\")\ndata_03463300$day &lt;- gsub(x = data_03463300$day, \n                          pattern = \" \",\n                          replacement = \"0\")\ndata_03463300$date &lt;- paste(data_03463300$yr, \n                            data_03463300$mon, \n                            data_03463300$day, sep = \"-\")\n\nwith(data_03463300, \n     plot(x = , y = , type = \"l\", \n          frame.plot = FALSE))"
  },
  {
    "objectID": "02_statistics.html#aggregation-and-summation",
    "href": "02_statistics.html#aggregation-and-summation",
    "title": "3  Statistical processing of hydrological data",
    "section": "3.4 Aggregation and summation",
    "text": "3.4 Aggregation and summation\nThese functions are based on grouping. In hydrology, the natural groups involve - stations/basins, decades/years/months, groundwater regions, etc.\n\n3.4.1 Box-plot\nWhen we want\n\n\nCode\nstation &lt;- read.delim(\"./data/6242910_Q_Day.Cmd.txt\", skip = 36, header = TRUE, sep = \";\", col.names = c(\"date\", \"time\", \"value\"))\nstation$date &lt;- as.Date(station$date, format = \"%Y-%m-%d\")\n\nstation_agg &lt;- aggregate(station$value ~ as.factor(data.table::month(station$date)), \n                         FUN = \"quantile\", \n                         probs = c(0.1, 0.25, 0.5, 0.75, 0.9))\nnames(station_agg) &lt;- c(\"Month\", \"Discharge\")\npar(font.main = 1, \n    adj = 0.1, \n    xaxs = \"i\", \n    yaxs = \"i\")\nboxplot(data = station_agg, \n        Discharge ~ Month, \n        main = \"Distribution of discharge in months\", \n        frame = FALSE, \n        ylim = c(0,20), \n        xlab = \"\", \n        ylab = bquote(Discharge), font.main = 1)\n\n\n\n\n\n\n\n3.4.2 Q-Q plot\n\n\nCode\npar(font.main = 1, \n    adj = 0.1, \n    xaxs = \"i\", \n    yaxs = \"i\")\nplot(quantile(rnorm(1000), probs = 1:100/100),\nquantile(rnorm(1000), probs = 1:100/100), frame = FALSE, pch = 20)\nabline(0, 1, col = \"red\")\n\n\n\n\n\n\n\n3.4.3 P-P plot\n\n\nCode\nlibrary(tidyverse)\n\n# ddd &lt;- read_fwf(\"data/6242910_Q_Day.Cmd.txt\", \n#                 skip = 36, \n#                 col_positions = fwf_widths(widths = c(8, 4, 4, 8, 2)), col_types = \"cccdc\")\nddd &lt;- read_csv2(\"data/6242910_Q_Day.Cmd.txt\", \n                skip = 36, \n                col_types = \"ccc\")\nddd |&gt; \n  set_names(c(\"Date\", \"D\", \"Discharge\")) |&gt; \n  mutate(Date = as.Date(Date, format = \"%Y-%m-%d\"), \n         Discharge = as.numeric(Discharge)) |&gt; \n  select(Date, Discharge) -&gt; ddd\n\n\n\nddd |&gt; \n  mutate(ri = rank(Discharge), \n         s = sd(Discharge), \n         q = pweibull(Discharge, shape = 2, scale = 1))\n\npdfunc &lt;- function(x, b = 0.44) {\n  n &lt;- length(x)\n  m &lt;- rank(x)\n  (m - b)/(n + 1 - 2*b)\n}\nplot(pdfunc(x &lt;- ddd$Discharge), type = \"l\")\n\nddd |&gt; \n  ggplot(aes(Date, Discharge)) +\n  geom_line()\n\nddd |&gt; \n  ggplot(aes(Discharge, ri = rank(Discharge / length(Discharge)))) +\n  geom_line()\n\nddd |&gt; \n  mutate(pi = rank(Discharge - 0.5)/length(Discharge))\n\n\nx &lt;- c(1, 6 ,6, 1, 2, 0)\nrank(x)\n\n\n\n\n3.4.4 Exceedance curve\n\n\n3.4.5 Frequency distribution curve"
  },
  {
    "objectID": "02_statistics.html#correlation-and-autocorrelation",
    "href": "02_statistics.html#correlation-and-autocorrelation",
    "title": "3  Statistical processing of hydrological data",
    "section": "3.5 Correlation and autocorrelation",
    "text": "3.5 Correlation and autocorrelation\n\\[\n\\rho_{X, Y} = \\dfrac{\\mathrm{cov}(X,Y)}{\\sigma_X\\sigma_Y}\n\\] \\[\n\\rho_{X,X}\n\\]"
  },
  {
    "objectID": "02_statistics.html#hydrological-indices",
    "href": "02_statistics.html#hydrological-indices",
    "title": "3  Statistical processing of hydrological data",
    "section": "3.6 Hydrological indices",
    "text": "3.6 Hydrological indices\n\n3.6.1 Runoff coefficient\nThe concept of runoff coefficient comes from the presumption, that over long-time period\n\\[\n\\varphi [-] = \\dfrac{R\\:[\\mathrm{mm}]}{P\\:[\\mathrm{mm}]}\n\\] where \\(R\\) represents runoff and \\(P\\) precipitation in long term.\n\n\n\n\nAddor, N., A. J. Newman, N. Mizukami, and M. P. Clark. 2017. “The CAMELS Data Set: Catchment Attributes and Meteorology for Large-Sample Studies.” Hydrology and Earth System Sciences 21 (10): 5293–313. https://doi.org/10.5194/hess-21-5293-2017."
  },
  {
    "objectID": "03_gis.html#whiteboxtools",
    "href": "03_gis.html#whiteboxtools",
    "title": "4  GIS in Hydrology",
    "section": "4.1 WhiteboxTools",
    "text": "4.1 WhiteboxTools\nWithin this introduction we use WhiteboxTools, a modern and advanced geospatial package, which contains ~450 tools and functions. The advantage of using WBT is in capability of inducing command line functions from within R."
  },
  {
    "objectID": "03_gis.html#watershed-delineation",
    "href": "03_gis.html#watershed-delineation",
    "title": "4  GIS in Hydrology",
    "section": "4.2 Watershed delineation",
    "text": "4.2 Watershed delineation\nThe process of delineation is the first step in basin description. One simply has to delineate the domain\nThe step-by-step process involves:\n- acquiring digital elevation model of area\n- pit and sink removal\n- flow accumulation calculation - flow direction calculation - outlet identification - delineation towards specified outlet"
  },
  {
    "objectID": "03_gis.html#geostatistics",
    "href": "03_gis.html#geostatistics",
    "title": "4  GIS in Hydrology",
    "section": "4.3 Geostatistics",
    "text": "4.3 Geostatistics\n\n4.3.1 River morphology statistics\nUnder the term river morphology we understand the description of the shape of river channels. Hydrologist utilize indices such as stream length, Strahler order"
  },
  {
    "objectID": "03_gis.html#session",
    "href": "03_gis.html#session",
    "title": "4  GIS in Hydrology",
    "section": "4.4 Session",
    "text": "4.4 Session\n\nStart with downloading DEM of specified area."
  },
  {
    "objectID": "04_interpolation.html#inverse-distance-weighting-idw",
    "href": "04_interpolation.html#inverse-distance-weighting-idw",
    "title": "5  Interpolation",
    "section": "5.1 Inverse distance weighting (IDW)",
    "text": "5.1 Inverse distance weighting (IDW)\n\\[\nZ_p = \\dfrac{\\sum\\limits_{i=1}^{n}\\dfrac{z_i}{d_i^P}}{\\sum\\limits_{i=1}^{n}\\dfrac{1}{d_i^P}}\n\\]\nLet’s create an arbitrary spatial domain\n\n\nCode\nlibrary(sf)          # Spatial Feature library for spatial data\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\nCode\nlibrary(scico)       # Scientific pallette collection called \"scico\"\n\nn &lt;- 25\ndom &lt;- data.frame(x     = runif(n, 0, 50),\n                  y     = runif(n, 0, 50), \n                  value = rnorm(n)) |&gt;\n  sf::st_as_sf(coords   = c(\"x\", \"y\"), crs = 4326)\nplot(dom, \n     nbreaks = 26,\n     pal = scico(n = 25,      \n                 palette   = \"davos\", \n                 end       = 0.9, \n                 direction = -1), \n     main      = \"Precipitation [mm]\", \n     pch       = 20,          # Point character selection\n     graticule = TRUE,        # Display graticules\n     axes      = TRUE,        # Display plot axes\n     bgc       = \"#f0f0f033\", # Background color\n     key.pos   = 1)           # Legend position\n\n\n\n\n\n\n\n\n\nWe need a grid of new points, which will represent the raster on which we want to recalculate\n\n\nCode\ngrid &lt;- st_make_grid(x = dom, \n                     cellsize = 2, \n                     crs = 4326) |&gt; \n  st_sf()\nplot(x = dom, \n     nbreaks = 26,\n     pal = scico(n = 25,      \n                 palette   = \"davos\", \n                 end       = 0.9, \n                 direction = -1), \n     main      = \"Precipitation [mm]\", \n     pch       = 20,          # Point character selection\n     graticule = TRUE,        # Display graticules\n     axes      = TRUE,        # Display plot axes\n     bgc       = \"#f0f0f033\", # Background color\n     key.pos   = 1, reset = FALSE)           # Legend position\nplot(x = st_geometry(grid), lwd = 0.1, add = TRUE, reset = TRUE)\n\n\n\n\n\nPoint location of Precipitation measuring stations\n\n\n\n\nNow we compute the distances between the points\n\n\nCode\ndistances &lt;- outer(\n  st_coordinates(grid)[, \"X\"], st_coordinates(dom)[, \"X\"],\n  st_coordinates(grid)[, \"Y\"], st_coordinates(dom)[, \"Y\"],\n  FUN = function(x1, x2, y1, y2) sqrt((x1 - x2)^2 + (y1 - y2)^2)\n)\n\n\nWarning in y1 - y2: longer object length is not a multiple of shorter object\nlength\n\n\n\n\nCode\npower &lt;- 2\nweighted_sum &lt;- apply(distances, 1, function(d) sum(dom$value / (d^power)))\n\n\n\nweights_sum &lt;- apply(distances, 1, function(d) sum(1 / (d^power)))\ninterpolated_values &lt;- weighted_sum / weights_sum\n\nresults &lt;- data.frame(\n  x = st_coordinates(grid)[, \"X\"],\n  y = st_coordinates(grid)[, \"Y\"],\n  value = interpolated_values) |&gt; \n  sf::st_as_sf(coords   = c(\"x\", \"y\"), crs = 4326)\n\nresults$value[which(is.nan(results$value))] &lt;- NA\n\nlibrary(ggplot2)\nggplot() +\n  geom_sf(data = results, aes(color = value)) +\n  geom_sf(data = grid, fill = NA) +\n  theme_bw() +\n  scale_color_scico(palette = \"davos\")\n\n\n\n\n\n\n\n\n\nCode\n# plot(x = results, \n#      nbreaks = 11,\n#      pal = scico(n = 10,\n#                  palette   = \"davos\",\n#                  end       = 0.9,\n#                  direction = -1),\n#      main      = \"Precipitation [mm] estimated by IDW\", \n#      pch       = 20,          # Point character selection\n#      graticule = TRUE,        # Display graticules\n#      axes      = TRUE,        # Display plot axes\n#      bgc       = \"#f0f0f033\", # Background color\n#      key.pos   = 1, reset = FALSE)           # Legend position\n# plot(x = st_geometry(grid), lwd = 0.1, add = TRUE, reset = TRUE)\n# \n# interpolated_matrix &lt;- matrix(interpolated_values, ncol = length(st_coordinates(grid)[, \"X\"]), nrow = length(st_coordinates(grid)[, \"Y\"]))\n# \n# nrow &lt;- length(unique(st_coordinates(results)[, \"Y\"]))\n# ncol &lt;- length(unique(st_coordinates(results)[, \"X\"]))\n# dim(results)\n# interpolated_matrix &lt;- matrix(results_df$interpolated_value, nrow = nrow, ncol = ncol, byrow = TRUE)\n# \n# plot(interpolated_matrix[, 1], interpolated_matrix[, 2])\n# contour(unique(st_coordinates(results)[, \"X\"]), unique(st_coordinates(results)[, \"Y\"]), z = results$value)\n# # Plot the interpolated values as an image\n# image(\n#   x = na.omit(st_coordinates(grid)[, \"X\"]),\n#   y = na.omit(st_coordinates(grid)[, \"Y\"]),\n#   z = interpolated_matrix,\n#   col = heat.colors(100),  # You can choose a color palette\n#   main = \"Interpolated Values\"\n# )"
  },
  {
    "objectID": "04_interpolation.html#gstat",
    "href": "04_interpolation.html#gstat",
    "title": "5  Interpolation",
    "section": "5.2 gstat",
    "text": "5.2 gstat\nThere are many libraries listed in CRAN geostatistics task view. One of these is called gstat, it was developed and is maintained by Edzer Pebesma, who is also behind the raster and terra packages.\n\n\nCode\nlibrary(gstat)\ngstat::idw(precipitation ~ x + y, locations = ~x+y)\n\n\n\n\nCode\npower &lt;- 2  # You can adjust the power parameter\nweighted_sum &lt;- apply(distances, 1, function(d) sum(df$value / (d^power)))\nweighted_sum"
  },
  {
    "objectID": "04_interpolation.html#krigging",
    "href": "04_interpolation.html#krigging",
    "title": "5  Interpolation",
    "section": "5.3 Krigging",
    "text": "5.3 Krigging\nAnother interpolation technique is called Krigging.\n\n5.3.1 Data\nLet’s use some artificial data set for this example as well.\n\\[\n\\mathrm{Distances} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}\\qquad \\mathrm{for}\\:i, j = 1,2, \\ldots,n\n\\]\n\\[\n\\mathrm{Differences} = \\Delta Z_{i,j} = Z_i - Z_j\\qquad \\mathrm{for}\n\\] ### Semivariogram calculation\n\\[\n\\gamma(h) = \\dfrac{1}{2N(h)}\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}\\Delta Z_{ij}(h)^2\n\\] where \\(\\gamma(h)\\) represents the semivariance lag at distance \\(h\\).\n\n\nCode\ndf &lt;- data.frame(\n  x = runif(10, 0, 50),\n  y = runif(10, 0, 50),\n  value = rnorm(10)\n)\nn &lt;- nrow(df)\ndist_matrix &lt;- as.matrix(dist(cbind(df$x, df$y)))\ndifferences_matrix &lt;- matrix(0, n, n)\n\nfor (i in 1:n) {\n  differences_matrix[i, ] &lt;- df$value - df$value[i]\n}\nlag_tol &lt;- 1  # Lag tolerance, you can adjust this\nmax_lag &lt;- 10  # Maximum lag distance, you can adjust this\n\nsemivariance &lt;- rep(0, max_lag / lag_tol)\nnum_pairs &lt;- rep(0, max_lag / lag_tol)\n\nfor (i in 1:(n - 1)) {\n  for (j in (i + 1):n) {\n    distance_ij &lt;- sqrt((df$x[i] - df$x[j])^2 + (df$y[i] - df$y[j])^2)\n    lag &lt;- round(distance_ij / lag_tol)\n    if (lag &lt;= max_lag / lag_tol) {\n      semivariance[lag] &lt;- semivariance[lag] + (df$value[i] - df$value[j])^2\n      num_pairs[lag] &lt;- num_pairs[lag] + 1\n    }\n  }\n}\n\nsemivariance &lt;- semivariance / (2 * num_pairs)\n\nprediction_grid &lt;- expand.grid(\n  x = seq(0, 50, by = 1),\n  y = seq(0, 50, by = 1)\n)\n\nn_grid &lt;- nrow(prediction_grid)\nsemivariance_at_grid &lt;- rep(0, n_grid)\n\nfor (i in 1:n_grid) {\n  differences_to_data &lt;- df$value - prediction_grid[i, ]\n  distances_to_data &lt;- sqrt((df$x - prediction_grid[i, \"x\"])^2 + (df$y - prediction_grid[i, \"y\"])^2)\n  lag_values &lt;- round(distances_to_data / lag_tol)\n  semivariance_at_grid[i] &lt;- sum((differences_to_data)^2 / (2 * lag_values))\n}\n\nn_lags &lt;- length(semivariance)\nkriged_values &lt;- rep(0, n_grid)\n\nfor (i in 1:n_grid) {\n  kriging_weights &lt;- rep(0, n)\n  distances_to_data &lt;- sqrt((df$x - prediction_grid[i, \"x\"])^2 + (df$y - prediction_grid[i, \"y\"])^2)\n  lag_values &lt;- round(distances_to_data / lag_tol)\n  for (j in 1:n) {\n    if (lag_values[j] &lt;= n_lags) {\n      kriging_weights[j] &lt;- (semivariance[lag_values[j]] - semivariance_at_grid[i]) / semivariance[lag_values[j]]\n    }\n  }\n  # Check if there are valid weights to avoid replacement with length zero\n  if (any(kriging_weights != 0)) {\n    kriged_values[i] &lt;- sum(kriging_weights * df$value)\n  }\n}\n\n\n\n\n5.3.2 Now we have to state the varigram model. We will calculate the semivariogram\nto model the spatial covariance structure. Then calculate the pairwise distances and differences between data points.\n\n\nCode\nn &lt;- nrow(df)\ndist_matrix &lt;- as.matrix(dist(cbind(df$x, df$y)))\ndifferences_matrix &lt;- matrix(0, n, n)\n\nfor (i in 1:n) {\n  differences_matrix[i, ] &lt;- df$value - df$value[i]\n}"
  },
  {
    "objectID": "05_CN.html#footnotes",
    "href": "05_CN.html#footnotes",
    "title": "6  Curve number method",
    "section": "",
    "text": "https://edepot.wur.nl/183157↩︎"
  }
]