# Data interpolation

Variables which enter hydrological analysis like *temperature* or *precipitation* are measured **locally at point locations**. Many of these qualities are then recalculated on different domains 
using **interpolation** or **extrapolation** algorithms. We interpolate within the boundary given by the values which are known to us and extrapolate outside this boundary. Here the two common methods for interpolation are demonstrated. 

## Inverse distance weighting (IDW)

IDW is a simple **deterministic** interpolation method, which is also non-parametric. 
The interpolated points are calculated with a weighted average of the values which are at disposal.
The space is a weighted average of the distribution of points and the weight assigned to each point decrease with increasing distance from the interpolated point.

The point value $Z_p$ is calculated with the knowledge of $z_i$ and distance $d_i$ to the power of $P$.

$$
Z_p = \dfrac{\sum\limits_{i=1}^{n}\dfrac{z_i}{d_i^P}}{\sum\limits_{i=1}^{n}\dfrac{1}{d_i^P}}
$$
Since this method is very simpley, let's calculate it on our own, in a step-by-step manner. 

 1.   First we generate some point in the spatial domain, which will represent our measurements.
 2.   Next we create a new domain, spatially regular to which we will interpolate.
 3.   Finally we will perform the calculations and visualize the results.

### Random measurements generation

We will create 25 points in the space and also assign some coordinate reference system. 
These points will be used for both of the methods.

```{r, fig.align='center', message=FALSE}
library(sf)          # Spatial Feature library for spatial data
library(scico)       # Scientific pallette collection called "scico"

n <- 25 # <1>
dom <- data.frame(x = runif(n, 0, 50), # <1>
                  y = runif(n, 0, 50), # <1>
                  value = rnorm(n, mean = 5)) |> # <1>
  sf::st_as_sf(coords = c("x", "y"), # <2>
               crs = 4326) # <2>
plot(dom,  # <3>
     nbreaks = 26, # <3>
     pal = scico(n = 25,      # <3>
                 palette = "davos", # <3>
                 end = 0.9, # <3>
                 direction = -1), # <3>
     main = "Precipitation [mm]", # <3>  
     pch  = 20,          # Point character selection # <3>
     graticule = TRUE,   # Display graticules # <3>
     axes = TRUE,        # Display plot axes # <3>
     bgc = "#f0f0f033",  # Background color # <3> 
     key.pos = 1)        # Legend position # <3>
```
1. Create $25$ points with random values, which will serve as the computation origin,
2. store them as **SimpleFeatures** object with coordinate reference system via EPSG,
search in [https://epsg.io](https://epsg.io)
3. Specify `sf:::plot.sf()` function and use scientific colormaps from `scico` package.

We do have the original points, which are scarcely distributed across the domain. 
Now we need a grid of new points, which will represent the centroids of a raster, 
on which we want to recalculate.

```{r, fig.align='center', fig.cap='Point location of Precipitation measuring stations'}
grid <- st_make_grid(x = dom,  # <1>
                     cellsize = 2, # <1> 
                     crs = 4326) |> # <1>
  st_sf()
plot(x = dom, # <2>
     breaks = seq(min(dom$value), max(dom$value), length.out = 25), # <2>
     pal = scico(n = 24,      # <2>
                 palette   = "davos", # <2>
                 end       = 0.9, # <2>
                 direction = -1), # <2>
     main      = "Precipitation [mm]", # <2>
     pch       = 20,          # Point character selection # <2>
     graticule = TRUE,        # Display graticules # <2>
     axes      = TRUE,        # Display plot axes # <2>
     bgc       = "#f0f0f033", # Background color # <2>
     key.pos   = 1, reset = FALSE)           # Legend position # <2>
plot(x = st_geometry(grid), lwd = 0.1, add = TRUE, reset = TRUE) # <2>


```
1. Construct a regular grid using extreme points as boundary limits. 
2. Plot with the points.


Now we compute the distances between the points usint the `outer()` function

```{r, eval=TRUE, fig.align='center', fig.cap=''}
distances <- outer(
  st_coordinates(grid)[, "X"], st_coordinates(dom)[, "X"],
  st_coordinates(grid)[, "Y"], st_coordinates(dom)[, "Y"],
  FUN = function(x1, x2, y1, y2) sqrt((x1 - x2)^2 + (y1 - y2)^2)
)
```

and use these distances for missing values computation

```{r, eval=TRUE, fig.align='center', fig.ext='svg'}
power <- 2
weighted_sum <- apply(distances, 1, function(d) sum(dom$value / (d^power)))



weights_sum <- apply(distances, 1, function(d) sum(1 / (d^power)))
interpolated_values <- weighted_sum / weights_sum

results <- data.frame(
  x = st_coordinates(grid)[, "X"],
  y = st_coordinates(grid)[, "Y"],
  value = interpolated_values) |> 
  sf::st_as_sf(coords   = c("x", "y"), crs = 4326)

results$value[which(is.nan(results$value))] <- NA


join <- st_join(grid, results)
```

At last we visualize the results. 

```{r, eval=TRUE, fig.align='center', fig.ext='svg'}
plot(x = join, 
     breaks = seq(min(results$value), max(results$value), length.out = 25),
     lwd = 0.001,
     pal = scico(24, 
                 palette = "davos",
                 end       = 0.9, 
                 direction = -1),
     main      = "Precipitation [mm]", 
     #pch       = 20,          # Point character selection
     graticule = TRUE,        # Display graticules
     axes      = TRUE,        # Display plot axes
     bgc       = "#f0f0f033", # Background color
     key.pos   = 1, reset = FALSE)           # Legend position
```

## `gstat`

There are many libraries listed in CRAN **geostatistics** task view. One of these is called gstat, it was developed and is maintained by Edzer Pebesma, who is also behind the `raster` and `terra` packages.
The gstat package contains functions no olny for interpolations.

```{r, eval=TRUE}
library(gstat)
krigged <- gstat::idw(formula = value ~ 1, locations = dom, newdata = grid)
krigged <- st_as_sf(krigged)
plot(krigged |> dplyr::select(var1.pred), 
     breaks = seq(min(krigged$var1.pred), max(krigged$var1.pred), length.out = 25),
     lwd = 0.001,
     pal = scico(24, 
                 palette = "davos",
                 end       = 0.9, 
                 direction = -1),
     main      = "Precipitation [mm]", 
     #pch       = 20,          # Point character selection
     graticule = TRUE,        # Display graticules
     axes      = TRUE,        # Display plot axes
     bgc       = "#f0f0f033", # Background color
     key.pos   = 1, reset = FALSE)
```

```{r, eval=FALSE}
power <- 2  # You can adjust the power parameter
weighted_sum <- apply(distances, 1, function(d) sum(df$value / (d^power)))
weighted_sum
```

## Krigging

Another interpolation technique is called **Krigging**. Opposing to the Inverse
Distance Weighted method. 

```{r, eval=FALSE}
krigged <- gstat::krige(formula = value ~ st_coordinates(dom)[, 1] + st_coordinates(dom)[, 2], 
                        locations = dom, 
                        newdata = grid)
krigged <- st_as_sf(krigged)
plot(krigged |> dplyr::select(var1.pred), 
     breaks = seq(min(krigged$var1.pred), max(krigged$var1.pred), length.out = 25),
     lwd = 0.001,
     pal = scico(24, 
                 palette = "davos",
                 end       = 0.9, 
                 direction = -1),
     main      = "Precipitation [mm]", 
     #pch       = 20,          # Point character selection
     graticule = TRUE,        # Display graticules
     axes      = TRUE,        # Display plot axes
     bgc       = "#f0f0f033", # Background color
     key.pos   = 1, reset = FALSE)
```


### Data

Let's use some artificial data set for this example as well.

$$
\mathrm{Distances} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}\qquad \mathrm{for}\:i, j = 1,2, \ldots,n
$$

$$
\mathrm{Differences} = \Delta Z_{i,j} = Z_i - Z_j\qquad \mathrm{for} 
$$ 

### Semivariogram calculation

```{r, fig.width=400}
show.vgms()
```

$$
\gamma(h) = \dfrac{1}{2N(h)}\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}\Delta Z_{ij}(h)^2
$$ where $\gamma(h)$ represents the semivariance lag at distance $h$.

```{r, eval=FALSE}
df <- data.frame(
  x = runif(10, 0, 50),
  y = runif(10, 0, 50),
  value = rnorm(10)
)
n <- nrow(df)
dist_matrix <- as.matrix(dist(cbind(df$x, df$y)))
differences_matrix <- matrix(0, n, n)

for (i in 1:n) {
  differences_matrix[i, ] <- df$value - df$value[i]
}
lag_tol <- 1  # Lag tolerance, you can adjust this
max_lag <- 10  # Maximum lag distance, you can adjust this

semivariance <- rep(0, max_lag / lag_tol)
num_pairs <- rep(0, max_lag / lag_tol)

for (i in 1:(n - 1)) {
  for (j in (i + 1):n) {
    distance_ij <- sqrt((df$x[i] - df$x[j])^2 + (df$y[i] - df$y[j])^2)
    lag <- round(distance_ij / lag_tol)
    if (lag <= max_lag / lag_tol) {
      semivariance[lag] <- semivariance[lag] + (df$value[i] - df$value[j])^2
      num_pairs[lag] <- num_pairs[lag] + 1
    }
  }
}

semivariance <- semivariance / (2 * num_pairs)

prediction_grid <- expand.grid(
  x = seq(0, 50, by = 1),
  y = seq(0, 50, by = 1)
)

n_grid <- nrow(prediction_grid)
semivariance_at_grid <- rep(0, n_grid)

for (i in 1:n_grid) {
  differences_to_data <- df$value - prediction_grid[i, ]
  distances_to_data <- sqrt((df$x - prediction_grid[i, "x"])^2 + (df$y - prediction_grid[i, "y"])^2)
  lag_values <- round(distances_to_data / lag_tol)
  semivariance_at_grid[i] <- sum((differences_to_data)^2 / (2 * lag_values))
}

n_lags <- length(semivariance)
kriged_values <- rep(0, n_grid)

for (i in 1:n_grid) {
  kriging_weights <- rep(0, n)
  distances_to_data <- sqrt((df$x - prediction_grid[i, "x"])^2 + (df$y - prediction_grid[i, "y"])^2)
  lag_values <- round(distances_to_data / lag_tol)
  for (j in 1:n) {
    if (lag_values[j] <= n_lags) {
      kriging_weights[j] <- (semivariance[lag_values[j]] - semivariance_at_grid[i]) / semivariance[lag_values[j]]
    }
  }
  # Check if there are valid weights to avoid replacement with length zero
  if (any(kriging_weights != 0)) {
    kriged_values[i] <- sum(kriging_weights * df$value)
  }
}


```

$$
\left[
\begin{matrix}
\mathbf{G} & \mathbf{j}\\
\mathbf{j^\prime} & \mathbf{0}\\
\end{matrix}
\right]
\left[
\begin{matrix}
\lambda\\
\mu_L
\end{matrix}
\right] = 
\left[
\begin{matrix}
\mathbf{g}\\
1
\end{matrix}
\right]
$$ Where $\mathbf{G}$ is a rectangular matrix containing the values of variograph for all doubles of measured points, $\mathbf{1}$ is a vector of weights s with values of

### Now we have to state the varigram model. We will calculate the semivariogram

to model the **spatial covariance** structure. Then calculate the pairwise distances and differences between data points.

```{r, eval=FALSE}
n <- nrow(df)
dist_matrix <- as.matrix(dist(cbind(df$x, df$y)))
differences_matrix <- matrix(0, n, n)

for (i in 1:n) {
  differences_matrix[i, ] <- df$value - df$value[i]
}
```
